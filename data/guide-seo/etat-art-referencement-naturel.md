---
title: "État de l'art du référencement naturel (SEO)"
publicationDate: "2023-09-21"
updateDate: "2023-09-25"
description: "Découvrez ce que vous allez apprendre au travers de ce guide sur le référencement naturel."
---

Tout d'abord, nous allons traiter des robots d'indexation, et du début du Search Engine Optimization. Ensuite, nous
verrons la révolution que Google a menée dans le milieu du Search Engine Optimization et que ce dernier est un véritable
jeu du chat et la souris. Puis, nous étudierons l'influence des sites sociaux sur le référencement naturel tout en
dressant un état actuel du référencement. Enfin, nous répertorierons les bonnes pratiques du référencement naturel et
nous aborderons la philosophie qui se cache derrière celui-ci.

## Robot d'indexation : le cœur du moteur de recherche

Au tout début, les moteurs de recherche étaient non pas maintenus par des algorithmes de manière automatique mais édités
par des humains.

Afin d'automatiser la chose, il a fallu créer des robots d'indexation, appelés "web crawlers" en anglais. De manière
simple, en passant de lien internet en lien internet, un robot d'indexation permet de parcourir automatiquement des
pages webs afin d'enregistrer leurs liens, de classer leurs contenus et leurs ressources tout en établissant une
cartographie détaillée des liens entre les sites internet.

Les premiers web crawlers ont vu le jour en 1993 et ils furent quatre : World Wide Web Wanderer, Jump Station, World
Wide Web Worm et RBSE Spider. Ces premiers crawlers avaient pour but de collecter des informations et des statistiques
sur le web à partir d'une liste de liens.

L'année suivante, le 20 Avril 1994, Brian PINKERTON créa le premier robot d'indexation capable d'indexer 15 pages webs
complètes de manière simultanée. De cette façon, l'utilisateur avait un top 15 des résultats de recherche. WebCrawler
introduisit aussi les concepts de politesse et liste noire.

La politesse d'un robot d'indexation est le fait de respecter le site sur lequel le robot est en train de faire son
travail. Il doit notamment ne pas surcharger le site qu'il en train de crawler, être identifiable et respecter les choix
des administrateurs du site en termes d'exploration par un crawler.

Quant à la liste noire, comme son nom l'indique, c'est une liste sur laquelle sont inscrits des sites qu'il ne faut pas
référencer lorsqu'ils traitent de sujets sensibles comme le terrorisme ou la drogue.

Entre 1993 et 1994, il est estimé que le nombre de pages indexées est passé
de [110 000 à 2 millions](http://ssrg.eecs.uottawa.ca/docs/CASCON2013.pdf). Pendant ce temps, des moteurs de recherche à
but commercial, uniquement basés sur les résultats des web crawlers voient le jour. Nous pouvons notamment citer Excite,
qui a racheté WebCrawler en 1996, Lycos, ou encore AltaVista.

Cependant, dès 1997, il y avait plus de 100 millions de pages web. Or, indexer une page a un coût, notamment en termes
de ressources informatiques et il n'était pas possible pour les crawlers de faire face à la création croissante de pages
web, sachant que début 2003, il y
avait [6,7 milliards de pages web](https://www.sites.univ-rennes2.fr/urfist/internet_chiffres).

Pour faire face à ce problème de scalabilité, Sergei BRIN et Larry PAGE ont créé en Mars 1996 un robot d'indexation à
large échelle appelé Backrub, renommé Google en 1998. Ils apportèrent une nouvelle approche dans l'indexation des pages
avec l'algorithme de PageRank.

Le PageRank calcule la probabilité qu'a un utilisateur de visiter une page internet en prenant en considération de
nombreux facteurs comme le nombre de sites qui pointent vers la page. En ayant cette probabilité, Google simule alors
une visite arbitraire et visite la page autant de fois qu'un utilisateur l'aurait fait.

Cette technique a permis à Google de donner plus d'importances aux pages qualitatives, de réduire les ressources
utilisées pour les pages inintéressantes et d'obtenir des données récentes sur les pages web.

Par conséquent, ces optimisations ont permis à Google de produire des résultats de recherches plus pertinents et
d'indexer plus rapidement puisque près de [75 millions de pages](https://en.wikipedia.org/wiki/History_of_Google)
étaient déjà indexées en seulement cinq mois, le 29 Août 1996.

## Débuts du Search Engine Optimization

En 1996, Danny SULLIVAN, publia une étude nommée "A Webmaster's Guide To Search Engine". Ce papier fut l'un des
premiers à sortir sur le sujet qui fut à destination des webmasters.

En 1997, Danny créa le site Search Engine Watch, un site avec des astuces destinées aux webmasters afin d'avoir de bons
résultats sur les moteurs de recherche. C'est d'ailleurs probablement à cette même année que le terme "Search Engine
Optimization (SEO)" a vu le jour.

Ainsi, c'est à partir de ce moment que l'on peut dire que le SEO a commencé véritablement à exister et à prendre de
l'ampleur puisqu'en 1999, la première conférence dédiée à la recherche sur les moteurs de recherche est organisée.
Lancée, encore une fois par Danny SULLIVAN, la conférence avait pour thèmes:

1. Concevoir des sites compatibles avec les moteurs de recherche : Astuces pour ceux qui construisent des sites pour les
   rendre plus compatibles avec les moteurs de recherche et obtenir du trafic naturellement ;
2. Meta tags : Différents experts expliquent leurs stratégies pour écrire des meta tags ;
3. Pages satellite : Apprendre les avantages et inconvénients de créer du contenu uniquement optimisé pour les moteurs
   de recherche ;

À l'époque, il y avait donc seulement trois axes de travail identifiés pour bien se positionner sur les moteurs de
recherche et cela relevait plus d'astuces que de réelles méthodes de travail pérennes.

## Révolution Google

Avant l'arrivée de Google, les moteurs de recherche classaient ainsi les sites uniquement en fonction du contenu des
pages d'un site. Ils étaient encore à leurs débuts et étaient facilement trompés.

En effet, afin de bien se positionner sur les moteurs de recherche, il fallait simplement répéter le mot-clé sur lequel
nous voulions nous positionner de très nombreuses fois dans la page et dans les meta tags d'un site.

Cependant, pour mettre fin à ces pratiques et augmenter la pertinence des résultats de recherche, Google a raisonné
d'une autre façon en utilisant son algorithme PageRank et son robot d'indexation. Ainsi, contrairement aux autres
moteurs de recherche de l'époque, Google ne regarda pas uniquement le contenu du site pour le classer mais collecta
aussi des données en dehors du site, comme le nombre de liens et la qualité des sources qui pointent vers le site. La
logique derrière cela est plutôt simple et revient à dire : "Si les gens parlent d'un site, alors ce site est
important, utile ou intéressant".

Étant les premiers à utiliser cette logique, Google a entamé le début d'une révolution dans la recherche et a posé les
bases de ce qu'allait être les moteurs de recherche 20 ans plus tard.

## Search Engine Optimization : le jeu du chat et la souris permanent

Les webmasters ont en permanence essayé de trouver des moyens d'être mieux référencés sur les moteurs de recherche. Le
but est de trouver une faille qui permet de mieux se classer tout en espérant que Google ne s'en rende pas compte.

Par exemple, en 2003, Google a lancé AdSense, qui permet de monétiser le trafic de son site internet via de l'affichage
publicitaire. C'est un bon moyen de se faire de l'argent avec son site mais il n'a fallu que peu de temps pour que
certaines personnes se rendent compte de l'opportunité que cela représentait.

Ainsi, tout une myriade de sites spécialement créés pour AdSense ont vu le jour. Le but était d'exceller en SEO afin de
drainer un maximum de trafic et d'utiliser ce trafic pour générer de l'argent avec AdSense. Néanmoins, afin d'exceller
en SEO, il a été utilisé des techniques comme le spam, le vol de contenu ou du contenu de très mauvaise qualité.

Autre exemple, en 2006, BMW s'est fait punir par Google pour l'utilisation d'une technique appelée "cloaking". Cette
technique consiste à présenter une version normale du site à un visiteur et une version sur-optimisée aux robots
d'indexation.

En ayant recours à cette technique connue depuis les débuts du SEO, BMW a enfreint la recommandation suivante : "Ne
trompez pas vos utilisateurs et ne présentez pas aux moteurs de recherche un contenu différent de celui que vous
affichez, qui est communément appelé "cloaking"" [traduction libre]. Ces recommandations émises par Google sont tout à
fait qualifiables de règles à respecter pour faire du SEO.

Par conséquent, en ayant enfreint cette règle, BMW a été réprimandé par Google. L'entreprise californienne a retiré BMW
des résultats de recherches pendant un certain temps et a remis leur PageRank a 0. Cela signifie qu'une fois les
sanctions levées, BMW a dû recommencer tout son travail de référencement naturel à zéro. C'est une punition sévère
sachant que ce travail peut être le fruit de nombreuses années.

Des exemples comme ceux-ci, il y en a eu tout au long de l'histoire des moteurs de recherche et ça continue encore
aujourd'hui. Toutefois, les moteurs de recherche apprennent de ces cas et mettent à jour leurs algorithmes afin que
cela ne se reproduise plus.

En effet, l'une des mises à jour les plus connues dans le milieu est nommée Florida. Sortie en 2003, elle est
considérée comme la première mise à jour majeure de Google et celle ayant changé à jamais le SEO. Le but de Google était
simple : faire disparaître les sites ne respectant pas ses règles et tenter de mettre fin aux techniques trompeuses,
précédemment citées, afin de mettre l'accent sur la qualité. La firme a plutôt bien réussi son coup puisque cette mise à
jour a supprimé
entre [50 et 98 %](https://web.archive.org/web/20040213034306/http:/www.searchengineposition.com/info/netprofit/floridaupdate.asp)
des résultats de recherche précédemment affichés. La moyenne étant à 72 %, cela signifie que du jour au lendemain, 72 %
des sites qui avaient l'habitude d'être dans le top 100 des recherches ont disparu. Cela a pu provoquer certaines
faillites de commerçants qui ont ensuite menacé de poursuivre Google en justice.

Florida était la première d'une longue série de mises à jour du moteur de recherche de Google. Nous pouvons notamment
citer Panda, Penguin, ou encore Pigeon. Globalement, ces mises à jour, qu'elles soient majeures ou mineures, ont un
unique objectif : améliorer les résultats de recherche en pénalisant ceux qui ont de mauvaises pratiques et en ajoutant
de nouveaux critères de classement comme la notion de géographie dans la recherche. C'est un jeu constant du chat et de
la souris puisque lorsqu'une faille est trouvée, Google finit toujours par sévir.

## Social Media Optimization : le SEO des sites sociaux

Le Search Engine Optimization (SMO) est globalement la même chose que le SEO, mais pour les sites sociaux. Cela consiste
donc à se placer dans les meilleures positions d'un site social et de tirer profit du trafic et de la visibilité que
celui-ci peut apporter.

En 2010, Danny SULLIVAN a eu la confirmation, de la part de Google et Bing, que Twitter et Facebook avaient une
influence sur
les [résultats de recherche](https://searchengineland.com/what-social-signals-do-google-bing-really-count-55389). Avant
ça, nous étions dans une
supposition [débutée en Août 2006](https://web.archive.org/web/20070820112850/http://blog.searchenginewatch.com/blog/060829-150053)
où le terme SMO est apparu pour la première fois dans la publication de blog
de [Rohit BHARGAVA](https://www.rohitbhargava.com/2006/08/5_rules_of_soci.html).

Depuis la confirmation de 2010, les moteurs de recherche ne cessent de s'appuyer de plus en plus sur les réseaux sociaux
comme Reddit, Twitter, Facebook, Pinterest ou Instagram et affichent même le contenu de certains directement dans les
résultats de recherche.

![Exemple de résultat de recherche Google avec le mot clé "#bordeaux".](/images/resultat_mot_cle_bordeaux.png)

Lorsqu'une page est "likée", "retweeté" ou partagée sur un réseau social, cela est assimilé à une sorte de vote
pour cette page. Ce vote est alors traité comme un signal par les moteurs de recherche, à l'instar d'une citation d'un
site par un autre site. Ceci afin d'encore mieux classer les sites.

Par conséquent, cela revient toujours à la même logique dont nous parlions précédemment : "Si les gens parlent d'un
site, alors ce site est important, utile ou intéressant ". Sauf que cette fois, cela est fait presque en temps réel et
à une échelle bien plus importante.

Par ailleurs, en plus d'avoir une influence sur le SEO, le SMO permet aussi d'acquérir une visibilité immense sur les
médias sociaux grâce aux phénomènes de tendances et autres systèmes de partages, "like" et "retweet". Ces systèmes
permettent alors d'avoir de la popularité beaucoup plus rapidement qu'avec le SEO, mais cette popularité est éphémère si
du contenu n'est pas régulièrement posté.

## État actuel du référencement naturel

Nous allons d'abord observer qu'au cours de son histoire, Google a établi une suprématie sur une partie du SEO. Puis,
nous verrons que de nouveaux moyens de faire du Search Engine Optimization et du Social Media Optimization sont apparus.

### Suprématie de Google

En 2019, Google a représenté, à lui seul, 92,63 % des parts de marché
des [moteurs de recherche](https://gs.statcounter.com/search-engine-market-share#yearly-2019-2019-bar). À travers le
monde, l'entreprise
comptabilise [75 866 recherches effectuées par seconde](https://www.internetlivestats.com/one-second/#google-band). Cela
représente près de 6,6 milliards de recherches par jour dont
environ [31 % des clics vont au premier résultat de recherche, 16 % au second et 10 % au troisième](https://www.advancedwebranking.com/ctrstudy/).

Ainsi, ces chiffres titanesques mettent en évidence que bien se positionner, en particulier sur Google, dans les
résultats de recherche naturels, est primordial pour augmenter sa visibilité en ligne. De plus, ces chiffres démontrent
la suprématie de Google sur la "Search". C'est pourquoi quand nous parlons de référencement et de moteur de
recherche, nous évoquons toujours la firme américaine.

### De nouveaux moyens pour le Search Engine Optimization

Au cours de ces dernières années, de nouveaux moyens de faire du Search Engine Optimization sont apparus.

#### Local

Tout d'abord, au milieu des années 2000, les moteurs de recherche se sont mis à améliorer leurs résultats de recherches
comportant une dimension géographique. Cela a ouvert une nouvelle dimension du référencement qui est appelé le
référencement local.

Aujourd'hui, ce type de référencement est devenu une vraie préoccupation pour les restaurateurs, hôteliers, artisans et
autres entreprises ayant un rayonnement local. En effet, lorsque l'on tape "restaurant" sur Google, trois
restaurants apparaissent en première position des résultats de recherche. De plus, associés aux noms des restaurants,
nous pouvons observer leurs notes, issues d'avis laissés par des internautes.

![Extrait d'un résultat de recherche pour le mot-clé "restaurant".](/images/recherche_mot-cle_restaurant.png)

Ici, l'enjeu pour les entreprises locales est tout d'abord de sortir en première position des résultats mais aussi
d'avoir la meilleure note possible. Le but évident est d'attirer le plus de clients possible. Pour l'exemple des
restaurants, sur Yelp pour chaque étoile gagnée sur une note moyenne, cela correspond à une augmentation du chiffre
d'affaires
de [5 à 9 %](https://www.modernrestaurantmanagement.com/the-impact-of-reviews-on-the-restaurant-market-infographic/) et
ces chiffres sont sans doute du même acabit concernant Google et ses avis. Cela démontre donc l'importance pour un
commerce local de se battre au quotidien pour ses étoiles, et donc son référencement.

#### Sites d'avis

Précédemment, nous parlions du site Yelp, spécialisé dans les avis de commerces locaux. Or, des sites comme Yelp, il en
existe des dizaines, pour différents domaines. Les plus connus sont sans doute TripAdvisor, La Fourchette ou Booking.

![Exemple des résultats de recherche avec la requête "10 meilleurs hôtels
bordeaux".](/images/requete_top10.png)

Par exemple, pour la requête ci-dessus, nous pouvons observer que le premier résultat est une annonce payante de
Booking. Ensuite, nous avons une carte avec une liste d'hôtels. Puis, il y a un lien vers TripAdvisor.

Le premier résultat naturel est donc celui de TripAdvisor. Si l'on clique dessus, nous tombons sur une page listant près
de 400 hôtels Bordelais classés par rapport qualité-prix. Par conséquent, encore une fois, la note et les avis prennent
tout leur importance car près de 80 % des internautes considèrent que les avis sont extrêmement importants ou très
importants afin
de [choisir un hôtel](https://www.modernrestaurantmanagement.com/the-impact-of-reviews-on-the-restaurant-market-infographic/).

De la même manière que les sites d'avis, Google et Facebook proposent de donner un avis et une note à des
établissements. Sur Google, ces avis sont visibles directement sur les résultats de recherche et sur Google Maps.

Ainsi, se classer sur ce genre de sites permet non seulement d'être plus visible sur les sites en eux-mêmes mais aussi
sur les moteurs de recherche provoquant alors une augmentation de la visibilité et donc du chiffre d'affaires.

#### Position zéro : le Graal à double tranchant

La position zéro est appelé "Google Answer Box" par Google ou "Featured Snippet" par les professionnels. C'est
un encart qui se situe au dessus du premier résultat de recherche d'où son surnom de position zéro, mais qui reste en
dessous des annonces payantes.

Avoir son site cité en position zéro est un signe de reconnaissance de la part de Google. En effet, cela signifie que le
moteur de recherche a une confiance élevée pour le site placé en position zéro et qu'il a estimé que c'était la réponse
la plus pertinente à la requête.

Cette position permet d'augmenter le trafic naturel de 100 à 500 % selon
des [recherches de Search Engine Land](https://searchengineland.com/seo-featured-snippets-leads-big-gains-236212) et
de [Hubspot](https://blog.hubspot.com/marketing/how-to-featured-snippet-box#sm.0000hu1eyb3asdb0vvl2okm237oao). Réussir à
se positionner dans une telle position est donc un enjeu très important.

Cependant, la position zéro est à double tranchant puisque si l'internaute trouve satisfaction tout de suite, en lisant
le résultat de la position zéro, et qu'il n'a pas besoin de lire la page du site dont la réponse est tirée, alors le
site n'y gagne pas à être à cette position.

![Exemple de position
zéro.](/images/position_zero.png)

Selon
une [étude menée par Jumpshot](https://sparktoro.com/blog/how-much-of-googles-search-traffic-is-left-for-anyone-but-themselves/)
aux États-Unis, au premier trimestre 2019, près de 49 % des recherches sur Google étaient des "zero-click searches",
littéralement, des recherches à zéro clic. Celles-ci ont augmenté de presque 12 % en trois ans, ce qui a provoqué une
baisse de plus de 13 % des clics liés aux résultats naturels, appelés organiques.

Certes, ces 50 % de recherches à zéro clic et leur croissance d'année en année sont inquiétantes pour les résultats
organiques. Néanmoins, certains comme Rand FISHKIN voient cela comme un panneau publicitaire ou une mention dans la
presse qui permet d'exposer son nom de marque à une cible tout en partageant de l'information et dont le suivi de trafic
est compliqué. Ainsi, avec cette vision, qu'elle amène ou non du trafic sur le site, la position zéro est une position
intéressante, signe de "bonne santé SEO" d'un site.

#### Voix

Avec la montée en puissance des assistants vocaux, au moyen d'un smartphone, d'une enceinte connectée ou d'autres
appareils, la recherche vocale devient, d'année en année, un véritable enjeu.

Selon Gartner, d'ici
2020, [30 % des sessions de navigation sur internet devraient être réalisées sans écran,](https://www.gartner.com/smarterwithgartner/gartner-predicts-a-virtual-world-of-exponential-change/)
c'est-à-dire avec la voix. Parmi ces sessions, il y aurait donc potentiellement une grande partie de recherche.

Néanmoins, aujourd'hui, mi 2020, nous semblons être encore assez loin de cette proportion. Mais, il est clair que d'ici
quelques années, cela deviendra une part importante de la recherche.

Quoi qu'il en soit, pour le moment, les assistants vocaux se contentent de taper la requête dans leur moteur de
recherche et lire un extrait du site ayant la première position.

Cependant, nous rencontrons la même problématique qu'évoquée précédemment avec la position zéro, à savoir si
l'utilisateur a sa réponse lors de la dictée, alors il ne visitera pas le site dont l'assistant vocal a tiré son savoir.

#### Image

En Février 2018, Google Images représentait 22,6 % des recherches réalisées aux États-Unis. Ce pourcentage révèle donc
une opportunité immense de se référencer sur des images.

![Graphique montrant la répartition des canaux de recherche sur internet en Février 2018, aux
États-Unis.](/images/repartition_canaux_recherche.png)

Graphique montrant la répartition des canaux de recherche sur internet en Février 2018, aux États-Unis.

De plus, avec l'arrivée d'outils comme Google Lens, qui permet de rechercher directement ce que l'on voit à partir de
l'appareil photo, cela ouvre de nouvelles perspectives pour la recherche d'images et donc le référencement lié à cette
recherche.

#### Vidéo

Dans les résultats de recherches, les moteurs de recherche ont l'habitude d'afficher des vidéos. Pour Google, elles sont
majoritairement tirées de YouTube.

![Exemple de vidéos mises en avant dans les résultats de recherche.](/images/video_resultats_recherches.png)

Exemple de vidéos mises en avant dans les résultats de recherche.

D'après la figure 8, visible ci-dessus, YouTube représente 4,3 % des recherches aux États-Unis en 2018. Ainsi, nous
pouvons presque considérer YouTube comme un moteur de recherche à part entière.

Qui dit moteur de recherche à part entière, dit Search Engine Optimization. En effet, il est aussi possible de
travailler son SEO sur YouTube afin que l'on soit plus facilement trouvable et ainsi mieux remonter dans les résultats
de recherche

Par conséquent, afin qu'une vidéo remonte dans les résultats de recherche, de la même manière qu'évoquée dans le
paragraphe sur les réseaux sociaux, celle-ci doit être bien positionnée sur le site hébergeur et bien répondre à la
requête de l'internaute.

### De nouveaux canaux de Social Media Optimization

Régulièrement, les médias sociaux mettent à disposition de leurs utilisateurs de nouvelles fonctionnalités offrant de
nouveaux moyens de faire du SMO. Par exemple, Instagram a ouvertement copié Snapchat en sortant ses "stories" en
Août 2016. Celles-ci ont ainsi apporté un nouveau moyen d'acquérir de la visibilité sur la plateforme.

De plus, l'apparition de nouvelles plateformes sociales sont une très bonne opportunité de devenir une référence sur ces
plateformes. En effet, d'après Guy KAWASAKI, "chaque nouvelle plateforme crée de nouvelles "stars"" car "il est
beaucoup plus facile d'obtenir de nombreux abonnés lorsqu'une plateforme vient d'être lancée". La raison à cela est
qu'il y a "moins de personnes à suivre et beaucoup moins de bruit". "Une nouvelle plateforme, c'est la conquête
d'un nouveau territoire : si vous voulez y attirer de nombreux abonnés, vous devez agir vite, et cela signifie agir
avant qu'il ne devienne évident que la plateforme aura du succès" (L'art des médias sociaux : Stratégies gagnantes
pour un usage professionnel).

Ainsi, les nouveaux canaux sociaux sont une très bonne opportunité si ceux-ci correspondent au ton et à la cible de
l'entreprise.

## Principales bonnes pratiques du Search Engine Optimization

Comme évoqué précédemment, il n'y a pas de méthodes miracles pour avoir un bon référencement naturel sur les moteurs de
recherche. Néanmoins, certains grands principes ont été établis au fur et à mesure de recherches et d'expérimentations
afin de poser des bases solides pour un référencement minimum. Toutefois, ce n'est pas pour autant binaire. Cela
signifie que même en appliquant ces bonnes pratiques, les résultats ne seront pas forcément au rendez-vous.

### Contenu de bonne qualité, créé pour les humains

Le contenu rédactionnel est la clé de voûte de tout référencement efficace. Il doit être rédigé de façon claire et
simple, sans perdre de vue qu'il est destiné en priorité aux humains et non aux moteurs de recherche. Ainsi, le but dans
la rédaction de contenu est d'apporter le maximum de valeur à l'internaute qui nous lira car si cela plaît à un humain,
les robots d'indexation sauront le repérer.

### Bonne architecture

Les humains et les robots d'indexation n'ont pas un temps illimité à allouer pour trouver des pages et des contenus. Il
faut donc que le site soit bien structuré afin que ni l'internaute, ni les robots ne se perdent sur le site. Par
conséquent, permettre que les pages soient facilement trouvables, c'est bénéfique pour tout le monde. En effet, en
permettant aux humains et aux robots de trouver plus facilement les pages d'un site, celles-ci se retrouvent plus
facilement et plus rapidement référencées.

### Mobile First

Depuis la fin 2016, Google a changé son fusil d'épaule en partant d'un constat simple : plus de la moitié des requêtes
sur Google sont effectuées sur mobile. Ainsi, l'entreprise a décidé de passer à une [indexation "mobile
first"](https://webmasters.googleblog.com/2016/11/mobile-first-indexing.html). C'est-à-dire qu'au lieu de scanner les
sites avec une vue d'ordinateur, les robots se restreignent à une vue mobile car certains sites sont différents entre
mobile et ordinateur. Ce changement permet alors à Google d'être en cohérence avec le type d'appareil majoritaire.

C'est pourquoi il est important qu'un site internet soit aussi bien adapté à un ordinateur qu'à un appareil mobile. À
préciser que cela devrait déjà être une banalité depuis une dizaine d'années.

### Vitesse de chargement du site

La vitesse de chargement du site est un autre paramètre important. En effet, de la même manière que pour l'architecture
du site, les robots d'indexation et les internautes n'ont pas un temps illimité à nous consacrer. Par conséquent, depuis
Juillet 2018, Google utilise
la [vitesse de chargement du site comme critère de classement](https://webmasters.googleblog.com/2018/01/using-page-speed-in-mobile-search.html).
Selon Google, ce critère pénalise les plus mauvais élèves, mais si le contenu de la page est excellent et pertinent, il
y aura exception et celle-ci ne se verra pas impactée.

Certes, les moteurs de recherche pénalisent les plus mauvais. Mais, les utilisateurs, eux, sont encore plus exigeants et
veulent qu'un site charge instantanément. En effet, 53 % des visiteurs abandonnent si un site consulté sur mobile
met [plus de trois secondes à charger](https://www.thinkwithgoogle.com/marketing-resources/data-measurement/mobile-page-speed-new-industry-benchmarks/).
C'est pourquoi, il est nécessaire qu'un site charge le plus rapidement possible.

### Expérience utilisateur agréable

Pour conclure sur ces bonnes pratiques, tous les éléments cités précédemment font partie de l'expérience utilisateur.
Chaque élément est une petite pièce qui permet de créer une expérience utilisateur agréable et satisfaisante.

D'ailleurs, en Mai 2020, Google a annoncé la prise en compte
de [nouveaux signaux](https://webmasters.googleblog.com/2020/05/evaluating-page-experience.html) pour le "page
experience ranking". En effet, deux nouveaux critères ont fait leur apparition : l'interactivité et la stabilité
visuelle. Ainsi, pour constituer une bonne expérience selon ces deux nouveaux critères,
la [page doit être interactive avant 100ms](https://developers.google.com/search/docs/guides/page-experience) et la
disposition de la page ne doit pas sauter lors du chargement de celle-ci.

Néanmoins, encore une fois, [Google rappel](https://webmasters.googleblog.com/2020/05/evaluating-page-experience.html)
que "bien que tous les composants de l'expérience de page soient importants, nous [Google] allons prioriser les pages
avec la meilleure information globale, même si certains aspects de l'expérience de page sont en-dessous de la moyenne.
Une bonne expérience de page ne remplace pas le fait d'avoir un contenu bon et pertinent. Cependant, dans les cas où il
y a de multiples pages qui ont un contenu similaire, l'expérience de page devient beaucoup plus importants pour la
visibilité dans les résultats de recherche" [traduction libre].

Finalement, malgré ces nouveaux critères, le contenu reste roi. Mais, il ne faut toutefois pas perdre de vue que
l'expérience utilisateur reflète aussi l'expérience qu'ont les robots d'indexation lors de leur passage sur un site
internet. Il est donc primordial de la soigner afin que cela influe positivement sur le référencement et les visites des
internautes.

## Règles du Social Media Optimization

De la même manière que le SEO, le SMO n'est pas une science exacte. Néanmoins, en 2006, cinq règles ont été établies
par [Rohit BHARGAVA](https://www.rohitbhargava.com/2006/08/5_rules_of_soci.html) pour mener à bien sa stratégie de
Social Media Optimization :

1. Augmenter les possibilités d'être partagé ;
2. Faire en sorte que la mise en favoris ou l'enregistrement pour lire plus tard soit simple ;
3. Récompenser les liens pointant vers son site ;
4. Aider son contenu à voyager sur internet ;
5. Encourager les reprises et les modifications de contenu ;

Depuis, en
2012, [Nick Burcher a ajouté 11 règles supplémentaires](https://books.google.fr/books?id=QQnp-duPmbMC&oi=fnd&pg=PR7&dq=%22Paid,+owned,+earned:+Maximising+marketing+returns+in+a+socially+connected+world%22&ots=ORqzzIAf84&sig=OfbiwPUdKAWTpNb4r0qYNdkzEDs&redir_esc=y#v=onepage&q) :

1. Être une ressource pour l'internaute, même si cela ne nous aide pas ;
2. Récompenser les internautes qui aident et qui apportent de la valeur ;
3. Participer en rejoignant la conversation ;
4. Savoir comment cibler son audience ;
5. Créer du contenu ;
6. Être réel ;
7. Ne pas oublier ses racines et être humble ;
8. Ne pas être effrayé d'essayer de nouvelles choses et rester frais ;
9. Développer sa stratégie SMO ;
10. Choisir ses tactiques SMO judicieusement ;
11. Faire en sorte que le SMO fasse partie des procédures et des bonnes pratiques.

Globalement, certaines règles citées précédemment servent aussi à améliorer le SEO. Ainsi, le SMO et le SEO sont
intrinsèquement liés tant par le contenu que la qualité.

## Référencement naturel : une "science artistique"

Le référencement naturel (SEO et SMO) est la catégorie de référencement qui demande le plus de travail, car c'est une
compétition méritocratique où le fair-play prime et qui n'a de fin que lorsque les adversaires capitulent. Cette
compétition est comparable à un jeu sans fin où le nombre de participants est décuplé, où la difficulté apparente est
augmentée et où les règles sont modifiées, tous les mois.

Par conséquent, à cause de ces perpétuelles évolutions, il ne peut pas y avoir de méthodes précises à respecter pour
arriver à un résultat souhaité. En ce sens, le référencement ne peut donc pas être une science exacte mais nous
pourrions plutôt le qualifier de "science artistique". D'une part, c'est une science, car il est nécessaire d'avoir
une rigueur scientifique pour réussir à devenir visible. D'autre part, c'est un art, car il est obligatoire de
travailler la rédaction, l'architecture et l'expérience utilisateur d'un site pour qu'il plaise, à la fois aux moteurs
de recherche et aux internautes.

Toutefois, depuis plus de 20 ans, l'essence du référencement, et plus particulièrement du SEO, reste la même, à savoir
proposer du contenu pertinent et d'excellente qualité aux internautes.

## Synthèse

Durant cette partie, nous avons appris que, grâce à ses robots d'indexation plus intelligents, Google avait révolutionné
le marché des moteurs de recherche tout en instaurant une suprématie sur ces derniers et donc sur le SEO.

Ensuite, nous avons vu que le SEO est un jeu permanent du chat et la souris entre Google et les webmasters. De plus,
nous avons montré que l'émergence du SMO vient changer la donne, même dans le SEO.

Puis, nous avons dressé un état actuel du référencement naturel et plus particulièrement des nouveaux moyens pour faire
du SEO et du SMO.

Enfin, nous avons vu qu'il existait des bonnes pratiques permettant de construire son référencement naturel, mais que
celles-ci ne fonctionnent pas à tous les coups, car le référencement naturel est une "science artistique".
