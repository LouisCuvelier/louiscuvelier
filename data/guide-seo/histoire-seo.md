---
title: "L'histoire du SEO : comment le référencement naturel est-il apparu ?"
publicationDate: "2023-09-20"
updateDate: "2023-09-25"
seoDescription: "En 1989, le web est créé, les moteurs de recherches n'existent pas, encore moins le SEO. Aujourd'hui, le référencement naturel est partout et indispensable."
introduction: "En 1989, le web est créé, les moteurs de recherches n'existent pas, et encore moins le SEO. Aujourd'hui, le référencement naturel est partout. Comment en sommes-nous arrivés là ? C'est ce que nous explorons dans ce chapitre."
cta: "Besoin d'aide pour le référencement de votre site ?"
---

## Robot d'indexation : le cœur du moteur de recherche

Au tout début, les moteurs de recherche n'étaient pas maintenus par des algorithmes de manière automatique, mais édités
par des humains.

Afin d'automatiser la chose, il a fallu créer des robots d'indexation, appelés "web crawlers". De manière
simple, en passant de lien internet en lien internet, un **robot d'indexation permet de parcourir automatiquement des
pages webs** afin d'enregistrer leurs liens, de classer leurs contenus et leurs ressources tout en établissant une
cartographie détaillée des liens entre les sites internet.

Les **premiers web crawlers ont vu le jour en 1993** et ils furent quatre : World Wide Web Wanderer, Jump Station, World
Wide Web Worm et RBSE Spider. Ces premiers crawlers avaient pour but de collecter des informations et des statistiques
sur le web à partir d'une liste de liens.

L'année suivante, le **20 Avril 1994, Brian PINKERTON créa le premier robot d'indexation capable d'indexer 15 pages webs
complètes de manière simultanée**, nommé WebCrawler. De cette façon, l'utilisateur avait un top 15 des résultats de
recherche. WebCrawler
introduisit aussi les concepts de politesse et de liste noire.

La politesse d'un robot d'indexation est le fait de respecter le site sur lequel le robot est en train de faire son
travail. Il doit notamment **ne pas surcharger le site qu'il en train de crawler, être identifiable et respecter les
choix
des administrateurs** du site en termes d'exploration par un crawler.

Quant à la liste noire, comme son nom l'indique, c'est une liste sur laquelle sont inscrits des sites qu'il ne faut pas
référencer lorsqu'ils traitent de sujets sensibles comme le terrorisme ou la drogue.

Entre 1993 et 1994, il est estimé que le nombre de pages indexées est passé
de [110 000 à 2 millions](http://ssrg.eecs.uottawa.ca/docs/CASCON2013.pdf). **Pendant ce temps, des moteurs de recherche
à
but commercial, uniquement basés sur les résultats des web crawlers voient le jour**. Nous pouvons notamment citer
Excite,
qui a racheté WebCrawler en 1996, Lycos, ou encore AltaVista.

Cependant, dès 1997, il y avait plus de 100 millions de pages web. Or, **indexer une page a un coût**, notamment en
termes
de ressources informatiques et il n'était pas possible pour les crawlers de faire face à la création croissante de pages
web, sachant que début 2003, il y
avait [6,7 milliards de pages web](https://www.sites.univ-rennes2.fr/urfist/internet_chiffres).

Pour faire face à ce problème de scalabilité, Sergei BRIN et Larry PAGE ont créé en Mars 1996 un robot d'indexation à
large échelle appelé Backrub, renommé Google en 1998. **Ils apportèrent une nouvelle approche dans l'indexation des
pages
avec l'algorithme de PageRank**.

Le **PageRank calcule la probabilité qu'a un utilisateur de visiter une page internet** en prenant en considération de
nombreux facteurs comme le nombre de sites qui pointent vers la page. En ayant cette probabilité, Google simule alors
une visite arbitraire et visite la page autant de fois qu'un utilisateur l'aurait fait.

Cette technique **a permis à Google de donner plus d'importances aux pages qualitatives, de réduire les ressources
utilisées pour les pages inintéressantes et d'obtenir des données récentes** sur les pages web.

Par conséquent, grâce à ces optimisations Google a pu produire des résultats de recherches plus pertinents et
indexer plus rapidement puisque près de **[75 millions de pages](https://en.wikipedia.org/wiki/History_of_Google)
étaient déjà indexées en seulement cinq mois**, le 29 Août 1996.

## Débuts du Search Engine Optimization

En 1996, Danny SULLIVAN, publia une étude nommée "A Webmaster's Guide To Search Engine". Ce papier ciblant les
webmasters fut l'un des
premiers sur le sujet.

En 1997, Danny créa le site Search Engine Watch. C'est un site avec des astuces destinées aux webmasters afin d'avoir de
bons
résultats sur les moteurs de recherche. **C'est d'ailleurs probablement à cette même année que le terme "Search Engine
Optimization (SEO)" a vu le jour**.

Ainsi, **c'est à partir de ce moment que l'on peut dire que le SEO a commencé véritablement à exister** et à prendre de
l'ampleur puisqu'en 1999, la première conférence dédiée à la recherche sur les moteurs de recherche est organisée.
Lancée, encore une fois par Danny SULLIVAN, la conférence avait pour thèmes:

1. Concevoir des sites compatibles avec les moteurs de recherche : Astuces pour ceux qui construisent des sites pour les
   rendre plus compatibles avec les moteurs de recherche et obtenir du trafic naturellement ;
2. Meta tags : Différents experts expliquent leurs stratégies pour écrire des meta tags ;
3. Pages satellite : Apprendre les avantages et inconvénients de créer du contenu uniquement optimisé pour les moteurs
   de recherche ;

À l'époque, il y avait donc seulement trois axes de travail identifiés pour bien se positionner sur les moteurs de
recherche et cela relevait plus d'astuces que de réelles méthodes de travail pérennes.

## Révolution Google

Avant l'arrivée de Google, les moteurs de recherche classaient ainsi les sites uniquement en fonction du contenu des
pages d'un site. **Ils étaient encore à leurs débuts et étaient facilement trompés**.

En effet, afin de bien se positionner sur les moteurs de recherche, **il fallait simplement répéter le mot-clé sur
lequel
nous voulions nous positionner de très nombreuses fois** dans la page et dans les meta tags d'un site.

Cependant, pour mettre fin à ces pratiques et augmenter la pertinence des résultats de recherche, **Google a raisonné
d'une autre façon en utilisant son algorithme PageRank et son robot d'indexation**. Ainsi, contrairement aux autres
moteurs de recherche de l'époque, Google ne regarda pas uniquement le contenu du site pour le classer, mais **collecta
aussi des données en dehors du site**, comme le nombre de liens et la qualité des sources qui pointent vers le site. La
logique derrière cela est plutôt simple et revient à dire : "**Si les gens parlent d'un site, alors ce site est
important, utile ou intéressant**".

Étant les premiers à utiliser cette logique, Google a entamé le début d'une révolution dans la recherche et a posé les
bases de ce qu'allait être les moteurs de recherche 20 ans plus tard.

## Search Engine Optimization : le jeu du chat et la souris permanent

Les webmasters ont en permanence essayé de trouver des moyens d'être mieux référencés sur les moteurs de recherche. **Le
but est de trouver une faille qui permet de mieux se classer tout en espérant que Google ne s'en rende pas compte**.

Par exemple, en 2003, Google a lancé AdSense, qui permet de monétiser le trafic de son site internet via de l'affichage
publicitaire. C'est un bon moyen de se faire de l'argent avec son site mais il n'a fallu que peu de temps pour que
certaines personnes se rendent compte de l'opportunité que cela représentait.

Ainsi, tout une myriade de sites spécialement créés pour AdSense ont vu le jour. **Le but était d'exceller en SEO afin
de
drainer un maximum de trafic et d'utiliser ce trafic pour générer de l'argent avec AdSense**. Néanmoins, afin d'exceller
en SEO, il a été utilisé des techniques comme le spam, le vol de contenu ou la production de contenu de très mauvaise
qualité.

Autre exemple, en 2006, **BMW s'est fait punir par Google pour l'utilisation d'une technique appelée "cloaking"**. Cette
technique consiste à présenter une version normale du site à un visiteur et une version sur-optimisée aux robots
d'indexation.

En ayant recours à cette technique connue depuis les débuts du SEO, BMW a enfreint la recommandation suivante : "Ne
trompez pas vos utilisateurs et ne présentez pas aux moteurs de recherche un contenu différent de celui que vous
affichez, qui est communément appelé "cloaking"" [traduction libre]. Ces recommandations émises par Google sont tout à
fait qualifiables de règles à respecter pour faire du SEO.

Par conséquent, en ayant enfreint cette règle, BMW a été réprimandé par Google. **L'entreprise californienne a retiré
BMW
des résultats de recherches pendant un certain temps et a remis leur PageRank a 0**. Cela signifie qu'une fois les
sanctions levées, BMW a dû recommencer tout son travail de référencement naturel à zéro. C'est une punition sévère
sachant que ce travail peut être le fruit de nombreuses années.

Des exemples comme ceux-ci, il y en a eu tout au long de l'histoire des moteurs de recherche et ça continue encore
aujourd'hui. Toutefois, **les moteurs de recherche apprennent de ces cas et mettent à jour leurs algorithmes afin que
cela ne se reproduise plus**.

En effet, l'une des mises à jour les plus connues dans le milieu est nommée Florida. Sortie en 2003, elle est
considérée comme **la première mise à jour majeure de Google et celle ayant changé à jamais le SEO**. Le but de Google
était
simple : faire disparaître les sites ne respectant pas ses règles et tenter de mettre fin aux techniques trompeuses,
précédemment citées, afin de mettre l'accent sur la qualité. La firme a plutôt bien réussi son coup puisque **cette mise
à
jour a supprimé
entre [50 et 98 %](https://web.archive.org/web/20040213034306/http:/www.searchengineposition.com/info/netprofit/floridaupdate.asp)
des résultats de recherche précédemment affichés**. La moyenne étant à 72 %, cela signifie que du jour au lendemain,
72 %
des sites qui avaient l'habitude d'être dans le top 100 des recherches ont disparu. Cela a pu provoquer certaines
faillites de commerçants qui ont ensuite menacé de poursuivre Google en justice.

Florida était la première d'une longue série de mises à jour du moteur de recherche de Google. Nous pouvons notamment
citer Panda, Penguin, ou encore Pigeon. Globalement, ces mises à jour, qu'elles soient majeures ou mineures, ont **un
unique objectif : améliorer les résultats de recherche en pénalisant ceux qui ont de mauvaises pratiques** et en
ajoutant
de nouveaux critères de classement comme la notion de géographie dans la recherche. C'est un jeu constant du chat et de
la souris puisque lorsqu'une faille est trouvée, Google finit toujours par sévir.

## Social Media Optimization : le SEO des sites sociaux

Le Search Engine Optimization (SMO) est globalement la même chose que le SEO, mais pour les sites sociaux. Cela consiste
donc à se placer dans les meilleures positions d'un site social et de tirer profit du trafic et de la visibilité que
celui-ci peut apporter.

En 2010, Danny SULLIVAN a eu la confirmation, de la part de Google et Bing, que X et Facebook avaient une
influence sur
les [résultats de recherche](https://searchengineland.com/what-social-signals-do-google-bing-really-count-55389). Avant
ça, nous étions dans une
supposition [débutée en Août 2006](https://web.archive.org/web/20070820112850/http://blog.searchenginewatch.com/blog/060829-150053)
où le terme SMO est apparu pour la première fois dans la publication de blog
de [Rohit BHARGAVA](https://www.rohitbhargava.com/2006/08/5_rules_of_soci.html).

Depuis la confirmation de 2010, les moteurs de recherche ne cessent de s'appuyer de plus en plus sur les réseaux sociaux
comme Reddit, X, Facebook, Pinterest ou Instagram et affichent même le contenu de certains directement dans les
résultats de recherche.

![Exemple de résultat de recherche Google avec le mot clé "#bordeaux".](/images/resultat_mot_cle_bordeaux.png)

Lorsqu'une page est "likée", "retweeté" ou partagée sur un réseau social, cela est assimilé à une sorte de vote
pour cette page. **Ce vote est alors traité comme un signal par les moteurs de recherche, à l'instar d'une citation d'un
site par un autre site**. Ceci afin d'encore mieux classer les sites.

Par conséquent, cela revient toujours à la même logique dont nous parlions précédemment : "Si les gens parlent d'un
site, alors ce site est important, utile ou intéressant". Sauf que cette fois, **cela est fait presque en temps réel et
à une échelle bien plus importante**.

Par ailleurs, en plus d'avoir une influence sur le SEO, le SMO permet aussi d'acquérir une visibilité immense sur les
médias sociaux grâce aux phénomènes de tendances et autres systèmes de partages, "like" et "retweet". Ces systèmes
permettent alors d'avoir de la popularité beaucoup plus rapidement qu'avec le SEO, mais cette popularité est éphémère si
du contenu n'est pas régulièrement posté.

## Conclusion

À l'inverse du SMO est relativement éphémère, **le référencement naturel reste le meilleur moyen d'obtenir une bonne
visibilité sur le long terme**.
Les moteurs de recherche ont mis en place des règles et mis à jour leurs algorithmes dans le but de fournir aux
internautes les contenus les plus
pertinents et de qualité possible, tout en luttant contre les abus.

Malgré ces mises à jour, le SEO a longtemps été dominé par certaines techniques discutables. Cependant, il s'oriente
aujourd'hui vers des
**pratiques plus éthiques et durables, basées sur la véritable création boostée par de la technique**. C'est ce que nous
abordons dans
le [chapitre suivant](/guide-seo/seo-aujourdhui-etat-art).
